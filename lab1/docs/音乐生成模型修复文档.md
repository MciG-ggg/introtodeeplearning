# éŸ³ä¹ç”Ÿæˆæ¨¡å‹ä¿®å¤æ–‡æ¡£

## ğŸ“‹ é—®é¢˜æ¦‚è¿°

æœ¬é¡¹ç›®åœ¨å®ç°åŸºäºLSTMçš„éŸ³ä¹ç”Ÿæˆæ¨¡å‹æ—¶é‡åˆ°äº†å¤šä¸ªå…³é”®é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æ­£å¸¸è®­ç»ƒå’Œç”ŸæˆéŸ³ä¹ã€‚é€šè¿‡å¯¹æ¯”æ ‡å‡†ç­”æ¡ˆï¼Œæˆ‘ä»¬è¯†åˆ«å¹¶ä¿®å¤äº†æ‰€æœ‰é—®é¢˜ã€‚

## ğŸ” é—®é¢˜è¯†åˆ«ä¸ä¿®å¤

### 1. æ¨¡å‹åˆå§‹åŒ–é”™è¯¯ âŒâ†’âœ…

**é—®é¢˜**:
```python
# é”™è¯¯çš„åˆå§‹åŒ–æ–¹å¼
model = LSTMModel(params['seq_length'], params['embedding_dim'], params['hidden_size'])
```

**æ ¹æœ¬åŸå› **: æ··æ·†äº† `vocab_size` å’Œ `seq_length` çš„æ¦‚å¿µï¼š
- `vocab_size`: è¯æ±‡è¡¨ä¸­ä¸åŒå­—ç¬¦çš„æ€»æ•° (83ä¸ªå­—ç¬¦)
- `seq_length`: æ¯ä¸ªè¾“å…¥åºåˆ—çš„é•¿åº¦ (100ä¸ªå­—ç¬¦)

**ä¿®å¤**:
```python
# æ­£ç¡®çš„åˆå§‹åŒ–æ–¹å¼
model = LSTMModel(vocab_size, params["embedding_dim"], params["hidden_size"])
```

**å½±å“**: è¿™ä¸ªé”™è¯¯å¯¼è‡´Embeddingå±‚å’ŒLinearå±‚çš„ç»´åº¦å®Œå…¨ä¸åŒ¹é…ï¼Œå¼•å‘CUDAç´¢å¼•è¶Šç•Œé”™è¯¯ã€‚

### 2. ä¼˜åŒ–å™¨é€‰æ‹©ä¸å½“ âŒâ†’âœ…

**é—®é¢˜**:
```python
# ä½¿ç”¨SGDä¼˜åŒ–å™¨
optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], momentum=0.9)
```

**ä¿®å¤**:
```python
# ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼ˆæ ‡å‡†ç­”æ¡ˆé€‰æ‹©ï¼‰
optimizer = torch.optim.Adam(model.parameters(), lr=params["learning_rate"])
```

**åŸå› **: Adamä¼˜åŒ–å™¨é€šå¸¸åœ¨åºåˆ—æ¨¡å‹è®­ç»ƒä¸­è¡¨ç°æ›´å¥½ï¼Œæ”¶æ•›æ›´å¿«ã€‚

### 3. æ–‡æœ¬ç”Ÿæˆå‡½æ•°å¤šä¸ªé”™è¯¯ âŒâ†’âœ…

#### 3.1 å­—ç¬¦å‘é‡åŒ–é”™è¯¯
**é—®é¢˜**:
```python
# é”™è¯¯çš„å‘é‡åŒ–æ–¹å¼
input_idx = vectorize_string(start_string)
input_idx = torch.tensor([input_idx], dtype=torch.long).to(device)
```

**ä¿®å¤**:
```python
# æ­£ç¡®çš„å‘é‡åŒ–æ–¹å¼
input_idx = [char2idx[s] for s in start_string]
input_idx = torch.tensor([input_idx], dtype=torch.long).to(device)
```

#### 3.2 éšè—çŠ¶æ€åˆå§‹åŒ–é”™è¯¯
**é—®é¢˜**:
```python
# æ‰¹æ¬¡å¤§å°é”™è¯¯
state = model.init_hidden(input_idx.size(0), device)  # å¯èƒ½æ˜¯6
```

**ä¿®å¤**:
```python
# æ­£ç¡®çš„æ‰¹æ¬¡å¤§å°
state = model.init_hidden(input_idx.size(0), device)  # åº”è¯¥æ˜¯1
```

#### 3.3 é¢„æµ‹é‡‡æ ·é€»è¾‘é”™è¯¯
**é—®é¢˜**:
```python
# å¤šä¸ªé”™è¯¯ï¼šç»´åº¦å¤„ç†ã€é‡‡æ ·æ–¹å¼ã€å­—ç¬¦è·å–
input_idx = torch.multinomial(predictions, num_samples=1)
text_generated.append(idx2char)  # é”™è¯¯ï¼
```

**ä¿®å¤**:
```python
# æ­£ç¡®çš„é‡‡æ ·é€»è¾‘
input_idx = torch.multinomial(torch.softmax(predictions[-1, :], dim=-1), num_samples=1)
text_generated.append(idx2char[input_idx.item()])
input_idx = input_idx.unsqueeze(0)  # ä¸ºä¸‹ä¸€æ¬¡è¿­ä»£å‡†å¤‡
```

### 4. è®­ç»ƒå¾ªç¯ä¸­çš„è°ƒè¯•ä»£ç  âŒâ†’âœ…

**é—®é¢˜**:
```python
# è®­ç»ƒä¸­æœ‰è°ƒè¯•æ‰“å°è¯­å¥
print("x_batch: ", x_batch.shape)
print("y_batch: ", y_batch.shape)
```

**ä¿®å¤**: ç§»é™¤è°ƒè¯•æ‰“å°è¯­å¥ï¼Œè®©è®­ç»ƒè¿‡ç¨‹æ›´æ¸…æ´ã€‚

## ğŸ¯ å…³é”®æ¦‚å¿µæ¾„æ¸…

### vocab_size vs seq_length
| å‚æ•° | å«ä¹‰ | æ•°å€¼ | ç”¨é€” |
|------|------|------|------|
| `vocab_size` | è¯æ±‡è¡¨å¤§å° | `len(vocab)` â‰ˆ 83 | Embeddingè¾“å…¥ç»´åº¦ï¼ŒLinearè¾“å‡ºç»´åº¦ |
| `seq_length` | åºåˆ—é•¿åº¦ | `params['seq_length']` = 100 | æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æ—¶é—´æ­¥æ•° |

### æ‰¹æ¬¡å¤§å° (Batch Size)
| é˜¶æ®µ | æ‰¹æ¬¡å¤§å° | åŸå›  |
|------|----------|------|
| è®­ç»ƒ | `params['batch_size']` = 8 | å¹¶è¡Œè®¡ç®—ï¼Œæé«˜æ•ˆç‡ |
| ç”Ÿæˆ | 1 | é€æ­¥ç”Ÿæˆï¼Œä¿æŒè¿è´¯æ€§ |

## ğŸš€ ä¿®å¤åçš„å·¥ä½œæµç¨‹

### 1. æ¨¡å‹å®šä¹‰
```python
model = LSTMModel(vocab_size, embedding_dim=256, hidden_size=1024)
```

### 2. è®­ç»ƒè¿‡ç¨‹
```python
optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)
for iter in range(3000):
    loss = train_step(x_batch, y_batch)
    # è®°å½•æŸå¤±ï¼Œä¿å­˜æ¨¡å‹
```

### 3. éŸ³ä¹ç”Ÿæˆ
```python
generated_text = generate_text(model, start_string="X", generation_length=1000)
```

### 4. éŸ³é¢‘è½¬æ¢å’Œä¿å­˜
```python
generated_songs = mdl.lab1.extract_song_snippet(generated_text)
for i, song in enumerate(generated_songs):
    waveform = mdl.lab1.play_song(song)
    wav_file_path = f"output_{i}.wav"
    write(wav_file_path, 88200, numeric_data)
    experiment.log_asset(wav_file_path)
```

## ğŸ“Š æ–‡ä»¶ä½ç½®

### ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶ä½ç½®ï¼š
- **å½“å‰å·¥ä½œç›®å½•**: `output_0.wav`, `output_1.wav`, ...
- **é¡¹ç›®ç›®å½•**: `/home/mcig/cs-self-learning/ai/introtodeeplearning/`
- **Comet ML**: ä¸Šä¼ åˆ°å®éªŒç•Œé¢çš„Audioå’ŒAssets & Artifactsé¡µé¢

### æ¨¡å‹æ£€æŸ¥ç‚¹ï¼š
- **ä½ç½®**: `./training_checkpoints/my_ckpt`
- **ä¿å­˜é¢‘ç‡**: æ¯100æ¬¡è¿­ä»£
- **æœ€ç»ˆæ¨¡å‹**: è®­ç»ƒç»“æŸåä¿å­˜

## âœ… éªŒè¯æ­¥éª¤

ä¿®å¤å®Œæˆåï¼ŒæŒ‰ä»¥ä¸‹é¡ºåºéªŒè¯ï¼š

1. **è¿è¡Œæ¨¡å‹æµ‹è¯•å•å…ƒæ ¼** - ç¡®è®¤ç»´åº¦æ­£ç¡®
2. **é‡æ–°è¿è¡Œè®­ç»ƒ** - ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åˆå§‹åŒ–
3. **ç”Ÿæˆæ–‡æœ¬** - æµ‹è¯•æ–‡æœ¬ç”Ÿæˆå‡½æ•°
4. **æ’­æ”¾éŸ³é¢‘** - ç¡®è®¤èƒ½æ­£å¸¸ç”Ÿæˆå’Œæ’­æ”¾éŸ³ä¹

## ğŸ‰ é¢„æœŸç»“æœ

ä¿®å¤åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- âœ… æˆåŠŸè®­ç»ƒæ¨¡å‹ï¼ŒæŸå¤±é€æ­¥ä¸‹é™
- âœ… ç”Ÿæˆåˆç†çš„ABCæ ¼å¼éŸ³ä¹
- âœ… è½¬æ¢å¹¶æ’­æ”¾ç”Ÿæˆçš„éŸ³ä¹
- âœ… åœ¨Comet MLä¸­æŸ¥çœ‹è®­ç»ƒæŒ‡æ ‡å’Œç”Ÿæˆçš„éŸ³é¢‘

## ğŸ“š å­¦åˆ°çš„æ•™è®­

1. **å‚æ•°åŒ¹é…çš„é‡è¦æ€§**: æ¨¡å‹çš„è¾“å…¥è¾“å‡ºç»´åº¦å¿…é¡»ä¸¥æ ¼åŒ¹é…æ•°æ®
2. **æ¦‚å¿µåŒºåˆ†**: vocab_sizeå’Œseq_lengthæ˜¯å®Œå…¨ä¸åŒçš„æ¦‚å¿µ
3. **é€æ­¥è°ƒè¯•**: å¤æ‚é—®é¢˜éœ€è¦é€æ­¥å®šä½å’Œä¿®å¤
4. **å‚è€ƒæ ‡å‡†ç­”æ¡ˆ**: åœ¨é‡åˆ°å›°éš¾æ—¶ï¼Œå¯¹æ¯”æ ‡å‡†å®ç°å¾ˆæœ‰å¸®åŠ©

---

**ä¿®å¤å®Œæˆæ—¶é—´**: 2025-10-28
**ä¿®å¤èŒƒå›´**: æ¨¡å‹åˆå§‹åŒ–ã€ä¼˜åŒ–å™¨ã€æ–‡æœ¬ç”Ÿæˆå‡½æ•°ã€è®­ç»ƒæµç¨‹
**çŠ¶æ€**: âœ… å…¨éƒ¨ä¿®å¤å®Œæˆï¼Œå¯æ­£å¸¸è¿è¡Œ