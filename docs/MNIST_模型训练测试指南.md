# MNISTæ‰‹å†™æ•°å­—è¯†åˆ«æ¨¡å‹è®­ç»ƒæµ‹è¯•å®Œæ•´æŒ‡å—

> **é¢å‘æ·±åº¦å­¦ä¹ åˆå­¦è€…çš„å®è·µæŒ‡å—**
> åŸºäº MIT 6.S191 Introduction to Deep Learning Lab2

---

## ğŸ“‹ ç›®å½•

1. [æ ¸å¿ƒé—®é¢˜å®šä¹‰](#1-æ ¸å¿ƒé—®é¢˜å®šä¹‰)
2. [åˆ†æ­¥è§£å†³æµç¨‹](#2-åˆ†æ­¥è§£å†³æµç¨‹)
   - [æ­¥éª¤1: æ•°æ®é¢„å¤„ç†å’ŒåŠ è½½](#æ­¥éª¤1-æ•°æ®é¢„å¤„ç†å’ŒåŠ è½½)
   - [æ­¥éª¤2: æ¨¡å‹æ¶æ„è®¾è®¡](#æ­¥éª¤2-æ¨¡å‹æ¶æ„è®¾è®¡)
   - [æ­¥éª¤3: è®­ç»ƒé…ç½®å’Œæ‰§è¡Œ](#æ­¥éª¤3-è®­ç»ƒé…ç½®å’Œæ‰§è¡Œ)
   - [æ­¥éª¤4: æ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°](#æ­¥éª¤4-æ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°)
   - [æ­¥éª¤5: ç»“æœåˆ†æå’Œä¼˜åŒ–](#æ­¥éª¤5-ç»“æœåˆ†æå’Œä¼˜åŒ–)
3. [å…³é”®ç»†èŠ‚è¯´æ˜](#3-å…³é”®ç»†èŠ‚è¯´æ˜)
4. [è°ƒè¯•æŠ€å·§æ±‡æ€»](#4-è°ƒè¯•æŠ€å·§æ±‡æ€»)
5. [é™„å½•: ä»£ç å®ç°å®Œæ•´å‚è€ƒ](#é™„å½•ä»£ç å®ç°å®Œæ•´å‚è€ƒ)

---

## 1. æ ¸å¿ƒé—®é¢˜å®šä¹‰

### ğŸ¯ ä¸»è¦ç›®æ ‡
**å¦‚ä½•å®ç°é«˜ç²¾åº¦æ‰‹å†™æ•°å­—è¯†åˆ«ï¼Ÿ**

- **æ•°æ®é›†**: MNIST (60,000è®­ç»ƒå›¾åƒ + 10,000æµ‹è¯•å›¾åƒ)
- **ä»»åŠ¡**: 10åˆ†ç±» (æ•°å­—0-9)
- **è¾“å…¥**: 28Ã—28åƒç´ ç°åº¦å›¾åƒ
- **è¾“å‡º**: æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ

### ğŸ“Š æ€§èƒ½æŒ‡æ ‡
- **ç›®æ ‡å‡†ç¡®ç‡**: >98% (æµ‹è¯•é›†)
- **è¯„ä¼°æŒ‡æ ‡**: å‡†ç¡®ç‡(Accuracy)ã€äº¤å‰ç†µæŸå¤±(Cross-Entropy Loss)
- **è®­ç»ƒæ•ˆç‡**: åˆç†çš„è®­ç»ƒæ—¶é—´å’Œè®¡ç®—èµ„æºæ¶ˆè€—

### ğŸ”§ æŠ€æœ¯æŒ‘æˆ˜
1. **ç‰¹å¾æå–**: å¦‚ä½•ä»åƒç´ ä¸­æå–æœ‰æ•ˆç‰¹å¾
2. **æ¨¡å‹é€‰æ‹©**: å…¨è¿æ¥ç½‘ç»œ vs å·ç§¯ç¥ç»ç½‘ç»œ
3. **è¶…å‚æ•°è°ƒä¼˜**: å­¦ä¹ ç‡ã€æ‰¹å¤§å°ã€ç½‘ç»œç»“æ„
4. **è¿‡æ‹Ÿåˆé¢„é˜²**: ç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›

### ğŸ’¡ è§£å†³æ–¹æ¡ˆæ¦‚è§ˆ
- **åŸºå‡†æ¨¡å‹**: ç®€å•å…¨è¿æ¥ç¥ç»ç½‘ç»œ (å‡†ç¡®ç‡~97%)
- **æ”¹è¿›æ¨¡å‹**: å·ç§¯ç¥ç»ç½‘ç»œCNN (å‡†ç¡®ç‡>99%)
- **è®­ç»ƒç­–ç•¥**: éšæœºæ¢¯åº¦ä¸‹é™ + äº¤å‰ç†µæŸå¤±
- **å®éªŒè¿½è¸ª**: Comet MLè¿›è¡Œè®­ç»ƒè¿‡ç¨‹ç›‘æ§

---

## 2. åˆ†æ­¥è§£å†³æµç¨‹

### æ­¥éª¤1: æ•°æ®é¢„å¤„ç†å’ŒåŠ è½½

#### ğŸ“ MNISTæ•°æ®é›†ç†è§£

**æ•°æ®é›†åŸºæœ¬ä¿¡æ¯:**
```python
# å›¾åƒå°ºå¯¸: 28Ã—28åƒç´  (ç°åº¦)
# è®­ç»ƒé›†: 60,000å¼ å›¾åƒ
# æµ‹è¯•é›†: 10,000å¼ å›¾åƒ
# ç±»åˆ«: 10ä¸ª (æ•°å­—0-9)
# æ•°æ®æ ¼å¼: PIL.Image â†’ torch.Tensor [1, 28, 28]
```

#### ğŸ”„ æ•°æ®å˜æ¢æµç¨‹

**å…³é”®ä»£ç è§£æ:**
```python
transform = transforms.Compose([
    # å°†å›¾åƒè½¬æ¢ä¸ºPyTorchå¼ é‡ï¼ŒåŒæ—¶å°†åƒç´ å€¼ä»[0,255]ç¼©æ”¾åˆ°[0,1]
    transforms.ToTensor()
])
```

**ToTensor()çš„ä½œç”¨æœºåˆ¶:**
1. **æ•°æ®ç±»å‹è½¬æ¢**: PIL.Image/numpy.ndarray â†’ torch.FloatTensor
2. **æ•°å€¼ç¼©æ”¾**: åƒç´ å€¼ä»0-255èŒƒå›´ç¼©æ”¾åˆ°0-1èŒƒå›´
3. **ç»´åº¦è°ƒæ•´**: HÃ—WÃ—C â†’ CÃ—HÃ—W (é€šé“ä¼˜å…ˆæ ¼å¼)
4. **å†…å­˜ä¼˜åŒ–**: è¿ç»­å†…å­˜å¸ƒå±€ï¼Œæé«˜GPUè®¡ç®—æ•ˆç‡

#### ğŸ“¦ DataLoaderå·¥ä½œæœºåˆ¶

**æ‰¹å¤„ç†é…ç½®:**
```python
BATCH_SIZE = 64
trainset_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
testset_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
```

**å…³é”®å‚æ•°è¯´æ˜:**
- **batch_size=64**: æ¯æ‰¹å¤„ç†64ä¸ªæ ·æœ¬ï¼Œå¹³è¡¡å†…å­˜ä½¿ç”¨å’Œæ¢¯åº¦ç¨³å®šæ€§
- **shuffle=True**: è®­ç»ƒé›†éšæœºæ‰“ä¹±ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›
- **shuffle=False**: æµ‹è¯•é›†ä¿æŒé¡ºåºï¼Œç¡®ä¿ç»“æœå¯é‡ç°

**DataLoaderæ ¸å¿ƒä¼˜åŠ¿:**
1. **å†…å­˜ç®¡ç†**: é¿å…ä¸€æ¬¡æ€§åŠ è½½å…¨éƒ¨æ•°æ®åˆ°å†…å­˜
2. **å¹¶è¡ŒåŠ è½½**: å¤šè¿›ç¨‹æ•°æ®é¢„å¤„ç†ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦
3. **è‡ªåŠ¨æ‰¹å¤„ç†**: è‡ªåŠ¨ç»„ç»‡æ•°æ®ä¸ºæ‰¹æ¬¡æ ¼å¼
4. **çµæ´»é‡‡æ ·**: æ”¯æŒå„ç§é‡‡æ ·ç­–ç•¥

#### ğŸ’» GPUå†…å­˜ç®¡ç†æœ€ä½³å®è·µ

**è®¾å¤‡é…ç½®ä»£ç :**
```python
# æ£€æŸ¥GPUå¯ç”¨æ€§
assert torch.cuda.is_available(), "Please enable GPU from runtime settings"

# è®¾ç½®è®¡ç®—è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ•°æ®å’Œæ¨¡å‹ç§»åŠ¨åˆ°GPU
images, labels = images.to(device), labels.to(device)
model = model.to(device)
```

**å†…å­˜ä¼˜åŒ–æŠ€å·§:**
1. **åŠæ—¶é‡Šæ”¾**: ä½¿ç”¨`torch.no_grad()`åœ¨æ¨ç†æ—¶ç¦ç”¨æ¢¯åº¦è®¡ç®—
2. **æ‰¹å¤§å°è°ƒæ•´**: æ ¹æ®GPUå†…å­˜é™åˆ¶è°ƒæ•´batch_size
3. **æ¢¯åº¦ç´¯ç§¯**: å¤§æ¨¡å‹æ—¶å¯ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡

### æ­¥éª¤2: æ¨¡å‹æ¶æ„è®¾è®¡

#### ğŸ§  å…¨è¿æ¥ç¥ç»ç½‘ç»œ (Fully Connected Network)

**åŸºç¡€æ¶æ„è§£æ:**
```python
class FullyConnectedModel(nn.Module):
    def __init__(self):
        super(FullyConnectedModel, self).__init__()
        self.flatten = nn.Flatten()           # å±•å¹³å±‚: 28Ã—28 â†’ 784
        self.fc1 = nn.Linear(28 * 28, 128)    # ç¬¬ä¸€å±‚: 784 â†’ 128
        self.relu = nn.ReLU()                 # æ¿€æ´»å‡½æ•°
        self.fc2 = nn.Linear(128, 10)         # è¾“å‡ºå±‚: 128 â†’ 10

    def forward(self, x):
        x = self.flatten(x)    # å±•å¹³è¾“å…¥å›¾åƒ
        x = self.fc1(x)        # ç¬¬ä¸€å±‚çº¿æ€§å˜æ¢
        x = self.relu(x)       # ReLUæ¿€æ´»å‡½æ•°
        x = self.fc2(x)        # è¾“å‡ºå±‚ï¼Œè¿”å›logits
        return x
```

**é€å±‚è¯¦è§£:**

1. **nn.Flatten()**:
   - **è¾“å…¥å½¢çŠ¶**: `[batch_size, 1, 28, 28]`
   - **è¾“å‡ºå½¢çŠ¶**: `[batch_size, 784]`
   - **ä½œç”¨**: å°†2Då›¾åƒå±•å¹³ä¸º1Då‘é‡ï¼Œé€‚é…å…¨è¿æ¥å±‚è¾“å…¥

2. **nn.Linear(784, 128)**:
   - **å‚æ•°æ•°é‡**: 784 Ã— 128 + 128 = 100,480ä¸ªå‚æ•°
   - **æ•°å­¦åŸç†**: `output = input Ã— weight + bias`
   - **ä½œç”¨**: å­¦ä¹ ä»åƒç´ åˆ°ç‰¹å¾çš„çº¿æ€§æ˜ å°„

3. **nn.ReLU()**:
   - **å…¬å¼**: `ReLU(x) = max(0, x)`
   - **ä¼˜ç‚¹**: è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œè®¡ç®—ç®€å•
   - **ä½œç”¨**: å¼•å…¥éçº¿æ€§ï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›

4. **nn.Linear(128, 10)**:
   - **å‚æ•°æ•°é‡**: 128 Ã— 10 + 10 = 1,290ä¸ªå‚æ•°
   - **è¾“å‡º**: 10ä¸ªç±»åˆ«çš„logits(æœªå½’ä¸€åŒ–çš„æ¦‚ç‡)
   - **ä½œç”¨**: æœ€ç»ˆåˆ†ç±»å†³ç­–

#### ğŸ¯ å·ç§¯ç¥ç»ç½‘ç»œ (CNN) - æ¨èæ–¹æ¡ˆ

**CNNæ¶æ„ä¼˜åŠ¿:**
- **å‚æ•°å…±äº«**: å·ç§¯æ ¸åœ¨å›¾åƒå„ä½ç½®å…±äº«å‚æ•°ï¼Œå¤§å¹…å‡å°‘å‚æ•°æ•°é‡
- **å¹³ç§»ä¸å˜æ€§**: å¯¹å›¾åƒä¸­ç›®æ ‡çš„ä½ç½®å˜åŒ–å…·æœ‰é²æ£’æ€§
- **ç‰¹å¾å±‚æ¬¡**: ä»ä½çº§è¾¹ç¼˜ç‰¹å¾åˆ°é«˜çº§è¯­ä¹‰ç‰¹å¾çš„è‡ªåŠ¨å­¦ä¹ 
- **ç©ºé—´ä¿¡æ¯ä¿ç•™**: ä¿æŒå›¾åƒçš„ç©ºé—´ç»“æ„ä¿¡æ¯

**CNNå®ç°è¯¦è§£ (åŸºäºå®é™…MITä»£ç ):**
```python
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        # ç‰¹å¾æå–éƒ¨åˆ† (åŸºäºMIT Lab2å®é™…æ¶æ„)
        self.features = nn.Sequential(
            # ç¬¬ä¸€ä¸ªå·ç§¯å±‚ + æ± åŒ–å±‚
            nn.Conv2d(in_channels=1, out_channels=24, kernel_size=3, stride=1),
            nn.MaxPool2d(kernel_size=2, stride=1),  # æ³¨æ„: stride=1è€Œä¸æ˜¯2!

            # ç¬¬äºŒä¸ªå·ç§¯å±‚ + æ± åŒ–å±‚
            nn.Conv2d(in_channels=24, out_channels=36, kernel_size=3, stride=1),
            nn.MaxPool2d(kernel_size=2),  # é»˜è®¤stride=2
        )

        # å±•å¹³å±‚
        self.flatten = nn.Flatten()

        # åˆ†ç±»éƒ¨åˆ†
        self.classifier = nn.Sequential(
            nn.Linear(36 * 5 * 5, 128),  # å…³é”®: 36*25 = 900, ä¸æ˜¯36*49
            nn.ReLU(),
            nn.Linear(128, 10)           # è¾“å‡º10ä¸ªç±»åˆ«
        )

    def forward(self, x):
        # ç‰¹å¾æå–
        x = self.features(x)

        # å±•å¹³
        x = self.flatten(x)

        # åˆ†ç±»
        x = self.classifier(x)
        return x

# é‡è¦: åœ¨å®ä¾‹åŒ–åæµ‹è¯•å°ºå¯¸
cnn_model = CNN()
sample_input = torch.randn(1, 1, 28, 28)  # æ¨¡æ‹Ÿè¾“å…¥
output = cnn_model(sample_input)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # åº”è¯¥æ˜¯ [1, 10]
```

**âš ï¸ å…³é”®å°ºå¯¸è®¡ç®—ä¿®æ­£:**

**é”™è¯¯ç†è§£ vs æ­£ç¡®è®¡ç®—:**
```python
# âŒ é”™è¯¯çš„è®¡ç®—æ–¹å¼ (åŸºäºæˆ‘çš„åˆå§‹ç†è§£)
# ç¬¬ä¸€ä¸ªæ± åŒ–: MaxPool2d(kernel_size=2, stride=2)
# 28 â†’ 14 â†’ 7

# âœ… æ­£ç¡®çš„è®¡ç®—æ–¹å¼ (åŸºäºMITå®é™…ä»£ç )
# ç¬¬ä¸€ä¸ªæ± åŒ–: MaxPool2d(kernel_size=2, stride=1)
# 28 â†’ 27 (å› ä¸ºstride=1)
# ç¬¬äºŒä¸ªæ± åŒ–: MaxPool2d(kernel_size=2)
# 27 â†’ 13 (27-2)/2 + 1 = 13.5 â‰ˆ 13
```

**å®é™…è¾“å‡ºå°ºå¯¸éªŒè¯:**
```python
def debug_cnn_dimensions():
    """è°ƒè¯•CNNç»´åº¦å˜åŒ–"""
    model = CNN()
    x = torch.randn(1, 1, 28, 28)

    print("è¾“å…¥:", x.shape)

    # ç¬¬ä¸€ä¸ªå·ç§¯å±‚
    x = nn.Conv2d(1, 24, 3, stride=1)(x)
    print("Conv1å:", x.shape)  # [1, 24, 26, 26] (28-3+1 = 26)

    # ç¬¬ä¸€ä¸ªæ± åŒ–å±‚ (stride=1!)
    x = nn.MaxPool2d(2, stride=1)(x)
    print("Pool1å:", x.shape)  # [1, 24, 25, 25] (26-2+1 = 25)

    # ç¬¬äºŒä¸ªå·ç§¯å±‚
    x = nn.Conv2d(24, 36, 3, stride=1)(x)
    print("Conv2å:", x.shape)  # [1, 36, 23, 23] (25-3+1 = 23)

    # ç¬¬äºŒä¸ªæ± åŒ–å±‚ (é»˜è®¤stride=2)
    x = nn.MaxPool2d(2)(x)
    print("Pool2å:", x.shape)  # [1, 36, 11, 11] (23-2)/2 + 1 = 11

    # å±•å¹³
    x = x.view(1, -1)
    print("å±•å¹³å:", x.shape)  # [1, 4356] (36*11*11 = 4356)

    # æ‰€ä»¥æ­£ç¡®çš„å…¨è¿æ¥å±‚è¾“å…¥åº”è¯¥æ˜¯ 4356ï¼Œä¸æ˜¯ 900!
```

**ä¿®æ­£åçš„CNNå®ç°:**
```python
class CorrectedCNN(nn.Module):
    def __init__(self):
        super(CorrectedCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 24, kernel_size=3, stride=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1)
        self.conv2 = nn.Conv2d(24, 36, kernel_size=3, stride=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2)  # é»˜è®¤stride=2

        # è®¡ç®—å±•å¹³åçš„å°ºå¯¸: 36 * 11 * 11 = 4356
        self.fc1 = nn.Linear(36 * 11 * 11, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool1(x)
        x = torch.relu(self.conv2(x))
        x = self.pool2(x)
        x = x.view(x.size(0), -1)  # å±•å¹³
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

**å·ç§¯å±‚å‚æ•°è¯¦è§£:**

1. **nn.Conv2d(1, 24, 3, 1, 1)**:
   - **in_channels=1**: è¾“å…¥é€šé“æ•°(ç°åº¦å›¾åƒ)
   - **out_channels=24**: è¾“å‡ºé€šé“æ•°(24ä¸ªç‰¹å¾å›¾)
   - **kernel_size=3**: å·ç§¯æ ¸å°ºå¯¸3Ã—3
   - **stride=1**: æ­¥é•¿ä¸º1
   - **padding=1**: å¡«å……1åƒç´ ï¼Œä¿æŒè¾“å‡ºå°ºå¯¸ä¸å˜
   - **å‚æ•°æ•°é‡**: 1Ã—24Ã—3Ã—3 + 24 = 240ä¸ªå‚æ•°

2. **nn.MaxPool2d(2, 2)**:
   - **kernel_size=2**: æ± åŒ–çª—å£2Ã—2
   - **stride=2**: æ­¥é•¿ä¸º2
   - **ä½œç”¨**: ä¸‹é‡‡æ ·ï¼Œå‡å°‘ç©ºé—´ç»´åº¦ï¼Œæå–ä¸»è¦ç‰¹å¾
   - **è¾“å‡ºå°ºå¯¸**: è¾“å…¥å°ºå¯¸çš„ä¸€åŠ

**å°ºå¯¸å˜åŒ–è®¡ç®—:**
```
è¾“å…¥: [1, 28, 28]
â†“ Conv2d(1â†’24, 3Ã—3, padding=1)
è¾“å‡º: [24, 28, 28]
â†“ MaxPool2d(2Ã—2, stride=2)
è¾“å‡º: [24, 14, 14]
â†“ Conv2d(24â†’36, 3Ã—3, padding=1)
è¾“å‡º: [36, 14, 14]
â†“ MaxPool2d(2Ã—2, stride=2)
è¾“å‡º: [36, 7, 7]
â†“ Flatten
è¾“å‡º: [1764] (36Ã—7Ã—7)
```

#### ğŸ“Š æ¨¡å‹å¯¹æ¯”åˆ†æ

| ç‰¹æ€§ | å…¨è¿æ¥ç½‘ç»œ | CNN |
|------|-----------|-----|
| **å‚æ•°æ•°é‡** | ~101,770 | ~244,010 |
| **æµ‹è¯•å‡†ç¡®ç‡** | ~97.9% | >99% |
| **è®­ç»ƒæ—¶é—´** | è¾ƒå¿« | ä¸­ç­‰ |
| **è¿‡æ‹Ÿåˆé£é™©** | è¾ƒé«˜ | è¾ƒä½ |
| **ç©ºé—´ä¿¡æ¯åˆ©ç”¨** | æ—  | å……åˆ†åˆ©ç”¨ |
| **å¯è§£é‡Šæ€§** | è¾ƒå¥½ | ä¸­ç­‰ |

**é€‰æ‹©å»ºè®®:**
- **åˆå­¦è€…**: å…ˆå®ç°å…¨è¿æ¥ç½‘ç»œï¼Œç†è§£åŸºæœ¬åŸç†
- **è¿½æ±‚æ€§èƒ½**: é€‰æ‹©CNNï¼Œè·å¾—æ›´é«˜å‡†ç¡®ç‡
- **è®¡ç®—èµ„æºæœ‰é™**: å…¨è¿æ¥ç½‘ç»œå‚æ•°è¾ƒå°‘ï¼Œè®­ç»ƒæ›´å¿«

---

### æ­¥éª¤3: è®­ç»ƒé…ç½®å’Œæ‰§è¡Œ

#### âš™ï¸ æŸå¤±å‡½æ•°é€‰æ‹©

**äº¤å‰ç†µæŸå¤± (Cross-Entropy Loss):**
```python
loss_function = nn.CrossEntropyLoss()
```

**ä¸ºä»€ä¹ˆé€‰æ‹©äº¤å‰ç†µæŸå¤±?**

1. **æ•°å­¦åŸç†**:
   ```
   CE(p, q) = -Î£ p(x) log(q(x))
   ```
   - p: çœŸå®æ¦‚ç‡åˆ†å¸ƒ (one-hotç¼–ç )
   - q: æ¨¡å‹é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ

2. **ä¼˜åŠ¿åˆ†æ**:
   - **æ¦‚ç‡è§£é‡Š**: è¾“å‡ºç›´æ¥å¯¹åº”æ¦‚ç‡åˆ†å¸ƒ
   - **æ¢¯åº¦ç‰¹æ€§**: å¯¹é”™è¯¯åˆ†ç±»çš„æƒ©ç½šæ›´ä¸¥é‡
   - **æ•°å€¼ç¨³å®š**: å†…ç½®Softmaxï¼Œé¿å…æ•°å€¼æº¢å‡º
   - **åˆ†ç±»ä»»åŠ¡**: ä¸“ä¸ºå¤šåˆ†ç±»è®¾è®¡

3. **ä¸æ¨¡å‹çš„é…åˆ**:
   - **è¾“å…¥**: æ¨¡å‹è¾“å‡ºçš„logits (æœªå½’ä¸€åŒ–åˆ†æ•°)
   - **å†…éƒ¨å¤„ç†**: è‡ªåŠ¨åº”ç”¨Softmaxå‡½æ•°
   - **è¾“å‡º**: æ ‡é‡æŸå¤±å€¼

#### ğŸš€ ä¼˜åŒ–å™¨é…ç½®

**éšæœºæ¢¯åº¦ä¸‹é™ (SGD):**
```python
optimizer = optim.SGD(model.parameters(), lr=0.1)
```

**SGDå‚æ•°è§£æ:**
- **lr=0.1**: å­¦ä¹ ç‡ï¼Œæ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿
- **model.parameters()**: æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
- **åŠ¨é‡å‚æ•°**: é»˜è®¤ä¸º0ï¼Œå¯è®¾ç½®momentum=0.9åŠ é€Ÿæ”¶æ•›

**å­¦ä¹ ç‡é€‰æ‹©ç­–ç•¥:**
```python
# ä¸åŒå­¦ä¹ ç‡çš„è®­ç»ƒæ•ˆæœå¯¹æ¯”
learning_rates = [0.001, 0.01, 0.1, 0.5]
# 0.1: æ”¶æ•›é€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½éœ‡è¡
# 0.01: ç¨³å®šæ”¶æ•›ï¼Œæ¨èåˆå­¦è€…ä½¿ç”¨
# 0.001: æ”¶æ•›æ…¢ï¼Œä½†æ›´ç¨³å®š
```

**ä¼˜åŒ–å™¨å¯¹æ¯”:**
```python
# SGD vs Adam ä¼˜åŒ–å™¨
sgd_optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
adam_optimizer = optim.Adam(model.parameters(), lr=0.001)
```

**Adamä¼˜åŠ¿:**
- **è‡ªé€‚åº”å­¦ä¹ ç‡**: æ¯ä¸ªå‚æ•°ç‹¬ç«‹çš„å­¦ä¹ ç‡
- **åŠ¨é‡åŠ é€Ÿ**: ç»“åˆä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡
- **æ”¶æ•›ç¨³å®š**: å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ
- **æ¨èåœºæ™¯**: å¤æ‚æ¨¡å‹ï¼Œå¤§æ•°æ®é›†

#### ğŸ”„ è®­ç»ƒå¾ªç¯è¯¦è§£

**å®Œæ•´è®­ç»ƒå‡½æ•°:**
```python
def train(model, dataloader, criterion, optimizer, epochs):
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼

    for epoch in range(epochs):
        total_loss = 0
        correct_pred = 0
        total_pred = 0

        for images, labels in dataloader:
            # 1. æ•°æ®ç§»åŠ¨åˆ°GPU
            images, labels = images.to(device), labels.to(device)

            # 2. å‰å‘ä¼ æ’­
            outputs = model(images)

            # 3. æ¢¯åº¦æ¸…é›¶ (å…³é”®æ­¥éª¤!)
            optimizer.zero_grad()

            # 4. è®¡ç®—æŸå¤±
            loss = criterion(outputs, labels)

            # 5. åå‘ä¼ æ’­
            loss.backward()

            # 6. å‚æ•°æ›´æ–°
            optimizer.step()

            # 7. ç»Ÿè®¡æŒ‡æ ‡
            total_loss += loss.item() * images.size(0)
            predicted = torch.argmax(outputs, dim=1)
            correct_pred += (predicted == labels).sum().item()
            total_pred += labels.size(0)

        # 8. è®¡ç®—epochæŒ‡æ ‡
        avg_loss = total_loss / total_pred
        accuracy = correct_pred / total_pred
        print(f"Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")
```

**å…³é”®æ­¥éª¤è¯¦è§£:**

1. **model.train()**:
   - å¯ç”¨Dropoutã€BatchNormç­‰è®­ç»ƒä¸“ç”¨å±‚
   - æ¢¯åº¦è®¡ç®—å¼€å¯
   - ä¸model.eval()å¯¹åº”

2. **optimizer.zero_grad()**:
   - **å¿…è¦æ€§**: PyTorché»˜è®¤ç´¯ç§¯æ¢¯åº¦
   - **å¿˜è®°åæœ**: æ¢¯åº¦çˆ†ç‚¸ï¼Œè®­ç»ƒå´©æºƒ
   - **æœ€ä½³å®è·µ**: æ¯æ¬¡åå‘ä¼ æ’­å‰è°ƒç”¨

3. **loss.backward()**:
   - è®¡ç®—æŸå¤±å¯¹å„å‚æ•°çš„æ¢¯åº¦
   - ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†æœºåˆ¶
   - æ¢¯åº¦å­˜å‚¨åœ¨parameter.gradä¸­

4. **optimizer.step()**:
   - æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°
   - åº”ç”¨ä¼˜åŒ–ç®—æ³• (SGD/Adamç­‰)
   - å‚æ•°æ›´æ–°: `param = param - lr * grad`

#### ğŸ“ˆ è®­ç»ƒç›‘æ§

**å®æ—¶ç›‘æ§æŒ‡æ ‡:**
```python
# æŸå¤±å†å²è®°å½•
loss_history = []

# è¿›åº¦æ¡æ˜¾ç¤º
from tqdm import tqdm

for epoch in range(epochs):
    for images, labels in tqdm(dataloader, desc=f"Epoch {epoch+1}"):
        # ... è®­ç»ƒä»£ç  ...
        loss_history.append(loss.item())
```

**å®éªŒè¿½è¸ªé…ç½®:**
```python
import comet_ml

# åˆå§‹åŒ–å®éªŒ
comet_ml.init(project_name="MNIST_Experiment")
experiment = comet_ml.Experiment()

# è®°å½•æŒ‡æ ‡
experiment.log_metric("loss", loss_value, step=global_step)
experiment.log_metric("accuracy", accuracy, step=epoch)

# è®°å½•æ¨¡å‹å’Œå›¾è¡¨
experiment.log_model("cnn_model", model)
experiment.log_figure(figure=plt)
```

**ç›‘æ§è¦ç‚¹:**
1. **æŸå¤±ä¸‹é™**: åº”è¯¥æŒç»­ä¸‹é™ï¼Œæœ€ç»ˆè¶‹äºç¨³å®š
2. **å‡†ç¡®ç‡æå‡**: å•è°ƒé€’å¢ï¼Œè¾¾åˆ°å¹³å°æœŸ
3. **è®­ç»ƒé€Ÿåº¦**: æ¯ä¸ªepochçš„æ—¶é—´æ¶ˆè€—
4. **å†…å­˜ä½¿ç”¨**: GPU/CPUå†…å­˜å ç”¨æƒ…å†µ
5. **æ¢¯åº¦å¥åº·**: é¿å…æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

---

### æ­¥éª¤4: æ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°

#### ğŸ§ª æ¨¡å‹è¯„ä¼°æµç¨‹

**è¯„ä¼°å‡½æ•°è¯¦è§£:**
```python
def evaluate(model, dataloader, loss_function):
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    test_loss = 0
    correct_pred = 0
    total_pred = 0

    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(images)
            loss = loss_function(outputs, labels)

            # ç»Ÿè®¡
            test_loss += loss.item() * images.size(0)
            predicted = torch.argmax(outputs, dim=1)
            correct_pred += (predicted == labels).sum().item()
            total_pred += labels.size(0)

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_loss = test_loss / total_pred
    accuracy = correct_pred / total_pred
    return avg_loss, accuracy
```

**å…³é”®æ­¥éª¤è§£æ:**

1. **model.eval()**:
   - **ä½œç”¨**: å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
   - **å½±å“**:
     - ç¦ç”¨Dropoutå±‚ (æ‰€æœ‰ç¥ç»å…ƒéƒ½å‚ä¸è®¡ç®—)
     - BatchNormä½¿ç”¨è¿è¡Œç»Ÿè®¡é‡è€Œéæ‰¹æ¬¡ç»Ÿè®¡é‡
     - å…³é—­è®­ç»ƒç‰¹æœ‰è¡Œä¸º

2. **torch.no_grad()**:
   - **å†…å­˜èŠ‚çœ**: ä¸è®¡ç®—æ¢¯åº¦ï¼Œå‡å°‘å†…å­˜å ç”¨
   - **è®¡ç®—åŠ é€Ÿ**: è·³è¿‡åå‘ä¼ æ’­è®¡ç®—
   - **æ•°å€¼ç¨³å®š**: é¿å…æ¨ç†æ—¶çš„æ•°å€¼è¯¯å·®ç´¯ç§¯

3. **torch.argmax()**:
   - **åŠŸèƒ½**: è¿”å›æœ€å¤§å€¼çš„ç´¢å¼•
   - **åº”ç”¨**: ä»logitsä¸­æ‰¾åˆ°é¢„æµ‹ç±»åˆ«
   - **æ›¿ä»£**: å¯ä»¥ä½¿ç”¨`torch.softmax()`è·å–æ¦‚ç‡åˆ†å¸ƒ

#### ğŸ“Š æ€§èƒ½æŒ‡æ ‡è®¡ç®—

**å‡†ç¡®ç‡ (Accuracy):**
```python
accuracy = correct_predictions / total_predictions
```
- **å®šä¹‰**: æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•° / æ€»æ ·æœ¬æ•°
- **ä¼˜ç‚¹**: ç›´è§‚æ˜“æ‡‚ï¼Œé€‚ç”¨äºå¹³è¡¡æ•°æ®é›†
- **ç¼ºç‚¹**: å¯¹ç±»åˆ«ä¸å¹³è¡¡æ•æ„Ÿ

**æŸå¤±å€¼ (Loss):**
```python
avg_loss = total_loss / total_samples
```
- **æ„ä¹‰**: æ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„å·®å¼‚
- **ç”¨é€”**: ç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼Œæ¯”è¾ƒæ¨¡å‹æ€§èƒ½
- **ç‰¹æ€§**: è¿ç»­å€¼ï¼Œä¾¿äºæ¢¯åº¦ä¼˜åŒ–

**æ··æ·†çŸ©é˜µ (Confusion Matrix):**
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# è®¡ç®—æ··æ·†çŸ©é˜µ
cm = confusion_matrix(true_labels, predicted_labels)

# å¯è§†åŒ–
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
```

#### ğŸ” è¿‡æ‹Ÿåˆæ£€æµ‹

**è¿‡æ‹Ÿåˆç°è±¡è¯†åˆ«:**
```python
# è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”
train_acc = evaluate(model, train_loader, loss_fn)[1]
test_acc = evaluate(model, test_loader, loss_fn)[1]

gap = train_acc - test_acc
if gap > 0.05:  # 5%çš„å·®å¼‚é˜ˆå€¼
    print("âš ï¸ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆç°è±¡!")
    print(f"è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    print(f"å‡†ç¡®ç‡å·®è·: {gap:.4f}")
```

**è¿‡æ‹Ÿåˆè§£å†³ç­–ç•¥:**

1. **Dropoutå±‚**:
   ```python
   class CNNWithDropout(nn.Module):
       def __init__(self):
           super().__init__()
           self.conv1 = nn.Conv2d(1, 24, 3, padding=1)
           self.dropout = nn.Dropout(0.25)  # 25%çš„ç¥ç»å…ƒè¢«éšæœºä¸¢å¼ƒ
           self.fc1 = nn.Linear(36 * 7 * 7, 128)
           self.dropout_fc = nn.Dropout(0.5)  # å…¨è¿æ¥å±‚ä½¿ç”¨æ›´é«˜dropoutç‡
   ```

2. **æ•°æ®å¢å¼º**:
   ```python
   transform = transforms.Compose([
       transforms.RandomRotation(10),      # éšæœºæ—‹è½¬Â±10åº¦
       transforms.RandomAffine(0, translate=(0.1, 0.1)),  # éšæœºå¹³ç§»
       transforms.ToTensor()
   ])
   ```

3. **æ—©åœç­–ç•¥ (Early Stopping)**:
   ```python
   best_test_acc = 0
   patience = 5  # å…è®¸è¿ç»­5æ¬¡æ— æ”¹è¿›
   wait_counter = 0

   for epoch in range(epochs):
       # ... è®­ç»ƒä»£ç  ...
       test_acc = evaluate(model, test_loader, loss_fn)[1]

       if test_acc > best_test_acc:
           best_test_acc = test_acc
           wait_counter = 0
           torch.save(model.state_dict(), 'best_model.pth')
       else:
           wait_counter += 1
           if wait_counter >= patience:
               print("æ—©åœ: éªŒè¯å‡†ç¡®ç‡ä¸å†æå‡")
               break
   ```

#### ğŸ“ˆ ç»“æœå¯è§†åŒ–

**å•å¼ å›¾åƒé¢„æµ‹å¯è§†åŒ–:**
```python
def visualize_prediction(model, image, true_label):
    model.eval()
    with torch.no_grad():
        output = model(image.unsqueeze(0))
        probabilities = torch.softmax(output, dim=1).squeeze()
        predicted = torch.argmax(probabilities).item()

    # åˆ›å»ºå­å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # æ˜¾ç¤ºå›¾åƒ
    ax1.imshow(image.squeeze(), cmap='gray')
    ax1.set_title(f'True: {true_label}, Predicted: {predicted}')
    ax1.axis('off')

    # æ˜¾ç¤ºæ¦‚ç‡åˆ†å¸ƒ
    ax2.bar(range(10), probabilities.cpu().numpy())
    ax2.set_xlabel('Digit')
    ax2.set_ylabel('Probability')
    ax2.set_title('Prediction Probabilities')
    ax2.set_xticks(range(10))

    plt.tight_layout()
    return fig
```

**æ‰¹é‡ç»“æœå±•ç¤º:**
```python
def show_predictions_grid(model, test_dataset, num_samples=16):
    indices = np.random.choice(len(test_dataset), num_samples, replace=False)

    fig, axes = plt.subplots(4, 4, figsize=(12, 12))
    for i, idx in enumerate(indices):
        image, label = test_dataset[idx]
        pred = predict_single(model, image)

        ax = axes[i // 4, i % 4]
        ax.imshow(image.squeeze(), cmap='gray')

        # é¢œè‰²ç¼–ç : æ­£ç¡®=è“è‰², é”™è¯¯=çº¢è‰²
        color = 'blue' if pred == label else 'red'
        ax.set_title(f'T:{label} P:{pred}', color=color)
        ax.axis('off')

    plt.tight_layout()
    return fig
```

---

### æ­¥éª¤5: ç»“æœåˆ†æå’Œä¼˜åŒ–

#### ğŸ¯ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

**1. è¶…å‚æ•°è°ƒä¼˜:**

**å­¦ä¹ ç‡è°ƒåº¦:**
```python
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau

# æ–¹æ¡ˆ1: å›ºå®šé—´éš”è¡°å‡
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

# æ–¹æ¡ˆ2: åŸºäºéªŒè¯æŸå¤±çš„è‡ªé€‚åº”è¡°å‡
scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for epoch in range(epochs):
    # ... è®­ç»ƒä»£ç  ...
    test_loss = evaluate(model, test_loader, loss_fn)[0]
    scheduler.step(test_loss)
```

**æ‰¹å¤§å°ä¼˜åŒ–:**
```python
# ä¸åŒæ‰¹å¤§å°çš„å¯¹æ¯”å®éªŒ
batch_sizes = [16, 32, 64, 128, 256]
results = {}

for batch_size in batch_sizes:
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # è®­ç»ƒæ¨¡å‹
    model = CNN().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # è®°å½•ç»“æœ
    results[batch_size] = train_and_evaluate(model, train_loader, test_loader)
```

**2. ç½‘ç»œæ¶æ„ä¼˜åŒ–:**

**æ›´æ·±çš„CNNæ¶æ„:**
```python
class ImprovedCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # ç‰¹å¾æå–éƒ¨åˆ†
        self.features = nn.Sequential(
            # ç¬¬ä¸€ä¸ªå·ç§¯å—
            nn.Conv2d(1, 32, 3, padding=1),
            nn.BatchNorm2d(32),           # æ‰¹æ ‡å‡†åŒ–
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),

            # ç¬¬äºŒä¸ªå·ç§¯å—
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
        )

        # åˆ†ç±»éƒ¨åˆ†
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x
```

**3. æ•°æ®å¢å¼ºç­–ç•¥:**
```python
# æ›´ä¸°å¯Œçš„æ•°æ®å¢å¼º
train_transform = transforms.Compose([
    transforms.RandomRotation(15),                    # éšæœºæ—‹è½¬Â±15åº¦
    transforms.RandomAffine(0, translate=(0.15, 0.15)), # éšæœºå¹³ç§»
    transforms.RandomAffine(0, shear=10),             # éšæœºå‰ªåˆ‡
    transforms.RandomAffine(0, scale=(0.9, 1.1)),     # éšæœºç¼©æ”¾
    transforms.RandomHorizontalFlip(p=0.5),          # æ°´å¹³ç¿»è½¬ (é€‚ç”¨äºéæ•°å­—æ•°æ®)
    transforms.ToTensor(),
    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)) # éšæœºæ“¦é™¤
])

test_transform = transforms.Compose([
    transforms.ToTensor()  # æµ‹è¯•é›†ä¸åšæ•°æ®å¢å¼º
])
```

#### ğŸ“Š å®éªŒå¯¹æ¯”åˆ†æ

**ä¸åŒä¼˜åŒ–å™¨æ€§èƒ½å¯¹æ¯”:**
```python
# ä¼˜åŒ–å™¨å¯¹æ¯”å®éªŒ
optimizers = {
    'SGD': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),
    'Adam': optim.Adam(model.parameters(), lr=0.001),
    'RMSprop': optim.RMSprop(model.parameters(), lr=0.001),
    'Adagrad': optim.Adagrad(model.parameters(), lr=0.01)
}

results = {}
for name, optimizer in optimizers.items():
    model = CNN().to(device)
    train_acc, test_acc = train_with_optimizer(model, optimizer)
    results[name] = {'train_acc': train_acc, 'test_acc': test_acc}

# å¯è§†åŒ–ç»“æœ
plt.figure(figsize=(10, 6))
names = list(results.keys())
train_accs = [results[name]['train_acc'] for name in names]
test_accs = [results[name]['test_acc'] for name in names]

x = np.arange(len(names))
width = 0.35

plt.bar(x - width/2, train_accs, width, label='Train Accuracy')
plt.bar(x + width/2, test_accs, width, label='Test Accuracy')
plt.xlabel('Optimizer')
plt.ylabel('Accuracy')
plt.title('Optimizer Performance Comparison')
plt.xticks(x, names)
plt.legend()
plt.show()
```

**è®­ç»ƒæ›²çº¿åˆ†æ:**
```python
def plot_training_curves(loss_history, train_acc_history, test_acc_history):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # æŸå¤±æ›²çº¿
    ax1.plot(loss_history)
    ax1.set_xlabel('Iteration')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training Loss')
    ax1.set_yscale('log')  # å¯¹æ•°åæ ‡æ›´å¥½åœ°è§‚å¯Ÿæ”¶æ•›

    # å‡†ç¡®ç‡æ›²çº¿
    epochs = range(1, len(train_acc_history) + 1)
    ax2.plot(epochs, train_acc_history, 'b-', label='Train Accuracy')
    ax2.plot(epochs, test_acc_history, 'r-', label='Test Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Training and Test Accuracy')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    return fig
```

#### ğŸ† æœ€ç»ˆæ€§èƒ½ä¼˜åŒ–å»ºè®®

**æœ€ä½³å®è·µæ€»ç»“:**

1. **æ¨¡å‹é€‰æ‹©**: CNNæ˜æ˜¾ä¼˜äºå…¨è¿æ¥ç½‘ç»œï¼Œæ¨èä½¿ç”¨CNNæ¶æ„
2. **è¶…å‚æ•°é…ç½®**:
   - å­¦ä¹ ç‡: Adamä½¿ç”¨0.001ï¼ŒSGDä½¿ç”¨0.01-0.1
   - æ‰¹å¤§å°: 64-128ä¹‹é—´å¹³è¡¡å†…å­˜å’Œæ€§èƒ½
   - è®­ç»ƒè½®æ•°: 10-20è½®ï¼Œé…åˆæ—©åœç­–ç•¥
3. **æ­£åˆ™åŒ–æŠ€æœ¯**:
   - Dropout: 0.25-0.5ä¹‹é—´
   - BatchNorm: æå‡è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦
   - æ•°æ®å¢å¼º: æœ‰æ•ˆå‡å°‘è¿‡æ‹Ÿåˆ
4. **å®éªŒç®¡ç†**: ä½¿ç”¨Comet MLç­‰å·¥å…·è®°å½•å’Œåˆ†æå®éªŒ

**é¢„æœŸæ€§èƒ½æŒ‡æ ‡:**
- **å…¨è¿æ¥ç½‘ç»œ**: 97-98% æµ‹è¯•å‡†ç¡®ç‡
- **åŸºç¡€CNN**: 98-99% æµ‹è¯•å‡†ç¡®ç‡
- **ä¼˜åŒ–CNN**: >99% æµ‹è¯•å‡†ç¡®ç‡
- **è®­ç»ƒæ—¶é—´**: 5-15åˆ†é’Ÿ (GPUç¯å¢ƒ)

---

## 3. å…³é”®ç»†èŠ‚è¯´æ˜

### ğŸ”§ è¶…å‚æ•°é€‰æ‹©æŒ‡å—

#### å­¦ä¹ ç‡ (Learning Rate)
```python
# ä¸åŒå­¦ä¹ ç‡çš„é€‚ç”¨åœºæ™¯
learning_rates = {
    0.1: "SGDä¼˜åŒ–å™¨ï¼Œå¿«é€Ÿæ”¶æ•›ä½†å¯èƒ½éœ‡è¡",
    0.01: "SGDä¼˜åŒ–å™¨æ¨èå€¼ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§",
    0.001: "Adamä¼˜åŒ–å™¨æ ‡å‡†å€¼ï¼Œç¨³å®šæ”¶æ•›",
    0.0001: "å¾®è°ƒé˜¶æ®µï¼Œç²¾ç¡®ä¼˜åŒ–"
}
```

**å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥:**
```python
# ä½™å¼¦é€€ç«è°ƒåº¦
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)

# æŒ‡æ•°è¡°å‡
scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)

# å¤šæ­¥é•¿è¡°å‡
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)
```

#### æ‰¹å¤§å° (Batch Size)
| æ‰¹å¤§å° | å†…å­˜ä½¿ç”¨ | è®­ç»ƒæ—¶é—´ | æ¢¯åº¦ç¨³å®šæ€§ | æ¨èåœºæ™¯ |
|--------|----------|----------|------------|----------|
| 16 | ä½ | æ…¢ | ä¸ç¨³å®š | å†…å­˜å—é™ |
| 32 | ä¸­ç­‰ | ä¸­ç­‰ | ä¸€èˆ¬ | å¸¸è§„é€‰æ‹© |
| 64 | ä¸­ç­‰ | å¿« | è¾ƒç¨³å®š | **æ¨è** |
| 128 | é«˜ | è¾ƒå¿« | ç¨³å®š | GPUå……è¶³ |
| 256 | å¾ˆé«˜ | å¾ˆå¿« | å¾ˆç¨³å®š | å¤§å†…å­˜ |

#### ç½‘ç»œæ·±åº¦å’Œå®½åº¦
```python
# ä¸åŒè§„æ¨¡çš„CNNé…ç½®
configs = {
    'small': {'channels': [16, 32], 'fc_size': 128},
    'medium': {'channels': [24, 36], 'fc_size': 128},  # å½“å‰ä½¿ç”¨
    'large': {'channels': [32, 64, 128], 'fc_size': 256}
}
```

### ğŸ“Š æ•°æ®å¤„ç†æœ€ä½³å®è·µ

#### æ•°æ®æ ‡å‡†åŒ–
```python
# è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
def compute_dataset_stats(dataset):
    loader = DataLoader(dataset, batch_size=100, shuffle=False)
    mean = 0.
    std = 0.
    for data, _ in loader:
        batch_samples = data.size(0)
        data = data.view(batch_samples, data.size(1), -1)
        mean += data.mean(2).sum(0)
        std += data.std(2).sum(0)
    mean /= len(loader.dataset)
    std /= len(loader.dataset)
    return mean, std

# åº”ç”¨æ ‡å‡†åŒ–
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean, std)  # ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„æ•°æ®é›†ç»Ÿè®¡é‡
])
```

#### æ•°æ®åŠ è½½ä¼˜åŒ–
```python
# é«˜æ•ˆçš„æ•°æ®åŠ è½½é…ç½®
train_loader = DataLoader(
    train_dataset,
    batch_size=64,
    shuffle=True,
    num_workers=4,        # å¤šè¿›ç¨‹åŠ è½½
    pin_memory=True,      # å›ºå®šå†…å­˜ï¼ŒåŠ é€ŸGPUä¼ è¾“
    persistent_workers=True  # ä¿æŒå·¥ä½œè¿›ç¨‹æ´»è·ƒ
)
```

### ğŸ¯ æ¨¡å‹æ¶æ„è®¾è®¡åŸåˆ™

#### å·ç§¯å±‚è®¾è®¡æ¨¡å¼
```python
# å¸¸è§çš„å·ç§¯å—æ¨¡å¼
def conv_block(in_channels, out_channels, kernel_size=3, dropout=0.25):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2),
        nn.BatchNorm2d(out_channels),  # æ‰¹æ ‡å‡†åŒ–
        nn.ReLU(inplace=True),
        nn.Dropout2d(dropout),
        nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size//2),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(inplace=True)
    )
```

#### æ®‹å·®è¿æ¥ (Residual Connection)
```python
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.shortcut = nn.Sequential()
        if in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        residual = self.shortcut(x)
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.bn2(self.conv2(x))
        x += residual
        return F.relu(x)
```

---

## 4. è°ƒè¯•æŠ€å·§æ±‡æ€»

### ğŸ› å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

#### 1. ç»´åº¦ä¸åŒ¹é…é”™è¯¯
```python
# é”™è¯¯ä¿¡æ¯ç¤ºä¾‹:
# RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x784 and 784x128)

# è°ƒè¯•æ–¹æ³•:
def debug_tensor_shapes(model, sample_input):
    """è°ƒè¯•å¼ é‡ç»´åº¦å˜åŒ–"""
    print(f"è¾“å…¥å½¢çŠ¶: {sample_input.shape}")

    x = sample_input
    for name, layer in model.named_children():
        x = layer(x)
        print(f"{name:15}: {x.shape}")

# ä½¿ç”¨ç¤ºä¾‹
sample_batch = next(iter(train_loader))[0].to(device)
debug_tensor_shapes(model, sample_batch)
```

#### 2. GPUå†…å­˜ä¸è¶³
```python
# è§£å†³æ–¹æ¡ˆ1: å‡å°æ‰¹å¤§å°
BATCH_SIZE = 32  # ä»64å‡å°‘åˆ°32

# è§£å†³æ–¹æ¡ˆ2: æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
effective_batch_size = BATCH_SIZE * accumulation_steps

for i, (images, labels) in enumerate(train_loader):
    outputs = model(images)
    loss = criterion(outputs, labels) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# è§£å†³æ–¹æ¡ˆ3: æ¸…ç†GPUç¼“å­˜
torch.cuda.empty_cache()
```

#### 3. è®­ç»ƒä¸æ”¶æ•›
```python
# è¯Šæ–­å·¥å…·
def diagnose_training_issues(model, train_loader):
    """è¯Šæ–­è®­ç»ƒé—®é¢˜"""

    # æ£€æŸ¥æ¢¯åº¦
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            if grad_norm > 10 or grad_norm < 1e-6:
                print(f"âš ï¸ {name}: æ¢¯åº¦å¼‚å¸¸ {grad_norm:.2e}")

    # æ£€æŸ¥å‚æ•°æ›´æ–°
    with torch.no_grad():
        for name, param in model.named_parameters():
            param_norm = param.norm().item()
            if param_norm > 100 or param_norm < 1e-6:
                print(f"âš ï¸ {name}: å‚æ•°å¼‚å¸¸ {param_norm:.2e}")

# è§£å†³æ–¹æ¡ˆ
solutions = {
    "æ¢¯åº¦çˆ†ç‚¸": "å‡å°å­¦ä¹ ç‡ï¼Œä½¿ç”¨æ¢¯åº¦è£å‰ª",
    "æ¢¯åº¦æ¶ˆå¤±": "ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ï¼Œæ£€æŸ¥ç½‘ç»œæ·±åº¦",
    "å­¦ä¹ ç‡è¿‡å¤§": "é™ä½å­¦ä¹ ç‡ï¼Œä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦",
    "æ•°æ®é—®é¢˜": "æ£€æŸ¥æ•°æ®é¢„å¤„ç†å’Œæ ‡ç­¾æ­£ç¡®æ€§"
}
```

#### 4. æ¨¡å‹é¢„æµ‹é”™è¯¯
```python
# é¢„æµ‹è°ƒè¯•å·¥å…·
def debug_predictions(model, test_loader, num_samples=5):
    """è°ƒè¯•æ¨¡å‹é¢„æµ‹"""
    model.eval()

    with torch.no_grad():
        for i, (images, labels) in enumerate(test_loader):
            if i >= num_samples:
                break

            outputs = model(images.to(device))
            probabilities = torch.softmax(outputs, dim=1)
            predictions = torch.argmax(probabilities, dim=1)

            for j in range(min(len(images), 3)):
                true_label = labels[j].item()
                pred_label = predictions[j].item()
                probs = probabilities[j].cpu().numpy()

                print(f"æ ·æœ¬ {i*len(images)+j}:")
                print(f"  çœŸå®æ ‡ç­¾: {true_label}")
                print(f"  é¢„æµ‹æ ‡ç­¾: {pred_label}")
                print(f"  é¢„æµ‹æ¦‚ç‡: {probs}")
                print(f"  æœ€å¤§æ¦‚ç‡: {probs.max():.4f}")
                print()
```

### ğŸ› ï¸ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### 1. è®­ç»ƒåŠ é€Ÿ
```python
# æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for images, labels in train_loader:
    images, labels = images.to(device), labels.to(device)

    optimizer.zero_grad()

    # è‡ªåŠ¨æ··åˆç²¾åº¦
    with autocast():
        outputs = model(images)
        loss = criterion(outputs, labels)

    # ç¼©æ”¾åå‘ä¼ æ’­
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

#### 2. å†…å­˜ä¼˜åŒ–
```python
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
def print_memory_usage():
    if torch.cuda.is_available():
        print(f"GPUå†…å­˜å·²ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        print(f"GPUå†…å­˜æ€»é‡: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")

# æ¨¡å‹æ£€æŸ¥ç‚¹
def create_checkpoint(model, optimizer, epoch, loss, filepath):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, filepath)

# åŠ è½½æ£€æŸ¥ç‚¹
def load_checkpoint(model, optimizer, filepath):
    checkpoint = torch.load(filepath)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return checkpoint['epoch'], checkpoint['loss']
```

### ğŸ“ˆ å®éªŒç®¡ç†æœ€ä½³å®è·µ

#### 1. ç³»ç»ŸåŒ–å®éªŒè®°å½•
```python
import json
from datetime import datetime

class ExperimentTracker:
    def __init__(self, experiment_name):
        self.experiment_name = experiment_name
        self.results = {}
        self.start_time = datetime.now()

    def log_config(self, **kwargs):
        self.results['config'] = kwargs

    def log_metrics(self, epoch, **kwargs):
        if 'metrics' not in self.results:
            self.results['metrics'] = []
        self.results['metrics'].append({'epoch': epoch, **kwargs})

    def save_results(self, filepath):
        self.results['duration'] = str(datetime.now() - self.start_time)
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2)

# ä½¿ç”¨ç¤ºä¾‹
tracker = ExperimentTracker("cnn_experiment_v1")
tracker.log_config(
    model_type="CNN",
    learning_rate=0.001,
    batch_size=64,
    optimizer="Adam"
)

for epoch in range(epochs):
    # ... è®­ç»ƒä»£ç  ...
    tracker.log_metrics(epoch, train_loss=train_loss, test_acc=test_acc)

tracker.save_results("experiment_results.json")
```

#### 2. æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
```python
def save_model_with_metadata(model, filepath, **metadata):
    """ä¿å­˜æ¨¡å‹åŠå…ƒæ•°æ®"""
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': metadata.get('config', {}),
        'training_metrics': metadata.get('metrics', {}),
        'timestamp': datetime.now().isoformat(),
        'pytorch_version': torch.__version__,
    }, filepath)

def load_model_with_metadata(model_class, filepath):
    """åŠ è½½æ¨¡å‹åŠå…ƒæ•°æ®"""
    checkpoint = torch.load(filepath)
    model = model_class(**checkpoint['model_config'])
    model.load_state_dict(checkpoint['model_state_dict'])
    return model, checkpoint
```

---

## é™„å½•: ä»£ç å®ç°å®Œæ•´å‚è€ƒ

### ğŸ“‹ å®Œæ•´çš„MNIST CNNå®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import comet_ml

# ============================================
# 1. é…ç½®å’Œè¶…å‚æ•°
# ============================================
CONFIG = {
    'batch_size': 64,
    'learning_rate': 0.001,
    'epochs': 15,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'model_name': 'MNIST_CNN_v1'
}

print(f"ä½¿ç”¨è®¾å¤‡: {CONFIG['device']}")

# ============================================
# 2. æ•°æ®é¢„å¤„ç†å’ŒåŠ è½½
# ============================================
# æ•°æ®å¢å¼ºå’Œé¢„å¤„ç†
train_transform = transforms.Compose([
    transforms.RandomRotation(10),
    transforms.RandomAffine(0, translate=(0.1, 0.1)),
    transforms.ToTensor()
])

test_transform = transforms.Compose([
    transforms.ToTensor()
])

# ä¸‹è½½å’ŒåŠ è½½æ•°æ®é›†
train_dataset = torchvision.datasets.MNIST(
    root='./data', train=True, download=True, transform=train_transform
)
test_dataset = torchvision.datasets.MNIST(
    root='./data', train=False, download=True, transform=test_transform
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(
    train_dataset, batch_size=CONFIG['batch_size'],
    shuffle=True, num_workers=2, pin_memory=True
)
test_loader = DataLoader(
    test_dataset, batch_size=CONFIG['batch_size'],
    shuffle=False, num_workers=2, pin_memory=True
)

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")

# ============================================
# 3. æ¨¡å‹å®šä¹‰
# ============================================
class ImprovedCNN(nn.Module):
    def __init__(self):
        super(ImprovedCNN, self).__init__()

        # ç‰¹å¾æå–éƒ¨åˆ†
        self.features = nn.Sequential(
            # ç¬¬ä¸€ä¸ªå·ç§¯å—
            nn.Conv2d(1, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),

            # ç¬¬äºŒä¸ªå·ç§¯å—
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
        )

        # åˆ†ç±»éƒ¨åˆ†
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# åˆå§‹åŒ–æ¨¡å‹
model = ImprovedCNN().to(CONFIG['device'])
print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# ============================================
# 4. è®­ç»ƒé…ç½®
# ============================================
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', patience=3, factor=0.5, verbose=True
)

# Comet ML å®éªŒè¿½è¸ª
try:
    import comet_ml
    comet_ml.init(project_name="MNIST_Improved")
    experiment = comet_ml.Experiment()
    experiment.log_parameters(CONFIG)
    use_comet = True
except:
    print("Comet ML æœªé…ç½®ï¼Œè·³è¿‡å®éªŒè¿½è¸ª")
    use_comet = False

# ============================================
# 5. è®­ç»ƒå‡½æ•°
# ============================================
def train_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    progress_bar = tqdm(train_loader, desc="è®­ç»ƒä¸­")

    for images, labels in progress_bar:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'acc': f'{100.*correct/total:.2f}%'
        })

    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total
    return avg_loss, accuracy

# ============================================
# 6. è¯„ä¼°å‡½æ•°
# ============================================
def evaluate(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    avg_loss = total_loss / len(test_loader)
    accuracy = 100. * correct / total
    return avg_loss, accuracy

# ============================================
# 7. è®­ç»ƒå¾ªç¯
# ============================================
def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, config):
    train_losses = []
    train_accuracies = []
    test_losses = []
    test_accuracies = []

    best_test_acc = 0
    patience = 5
    patience_counter = 0

    for epoch in range(config['epochs']):
        print(f"\nEpoch {epoch+1}/{config['epochs']}")
        print("-" * 50)

        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, config['device'])

        # è¯„ä¼°
        test_loss, test_acc = evaluate(model, test_loader, criterion, config['device'])

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(test_loss)

        # è®°å½•æŒ‡æ ‡
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)
        test_losses.append(test_loss)
        test_accuracies.append(test_acc)

        # æ‰“å°ç»“æœ
        print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
        print(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}, æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
        print(f"å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")

        # Comet ML è®°å½•
        if use_comet:
            experiment.log_metrics({
                'train_loss': train_loss,
                'train_accuracy': train_acc,
                'test_loss': test_loss,
                'test_accuracy': test_acc,
                'learning_rate': optimizer.param_groups[0]['lr']
            }, epoch=epoch+1)

        # æ—©åœæ£€æŸ¥
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            patience_counter = 0
            torch.save(model.state_dict(), 'best_model.pth')
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"æ—©åœè§¦å‘! {patience} è½®æ— æ”¹è¿›")
                break

    print(f"\nè®­ç»ƒå®Œæˆ! æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
    return train_losses, train_accuracies, test_losses, test_accuracies

# ============================================
# 8. å¼€å§‹è®­ç»ƒ
# ============================================
if __name__ == "__main__":
    # è®­ç»ƒæ¨¡å‹
    history = train_model(
        model, train_loader, test_loader,
        criterion, optimizer, scheduler, CONFIG
    )

    train_losses, train_accuracies, test_losses, test_accuracies = history

    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    model.load_state_dict(torch.load('best_model.pth'))
    final_test_loss, final_test_acc = evaluate(model, test_loader, criterion, CONFIG['device'])
    print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_test_acc:.2f}%")

    # ============================================
    # 9. ç»“æœå¯è§†åŒ–
    # ============================================
    def plot_training_history(train_losses, train_accs, test_losses, test_accs):
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        # æŸå¤±æ›²çº¿
        epochs = range(1, len(train_losses) + 1)
        ax1.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±')
        ax1.plot(epochs, test_losses, 'r-', label='æµ‹è¯•æŸå¤±')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('è®­ç»ƒå’Œæµ‹è¯•æŸå¤±')
        ax1.legend()
        ax1.grid(True)

        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(epochs, train_accs, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡')
        ax2.plot(epochs, test_accs, 'r-', label='æµ‹è¯•å‡†ç¡®ç‡')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.set_title('è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡')
        ax2.legend()
        ax2.grid(True)

        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        plt.show()

        if use_comet:
            experiment.log_figure(figure=fig)

    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_training_history(train_losses, train_accuracies, test_losses, test_accuracies)

    # ç»“æŸ Comet å®éªŒ
    if use_comet:
        experiment.end()

    print("ğŸ‰ è®­ç»ƒå’Œè¯„ä¼°å®Œæˆ!")
```

---

**ğŸ“š æ€»ç»“**

æœ¬æŒ‡å—æä¾›äº†MNISTæ‰‹å†™æ•°å­—è¯†åˆ«çš„å®Œæ•´å®ç°æµç¨‹ï¼Œä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹ä¼˜åŒ–ï¼Œæ¶µç›–äº†æ·±åº¦å­¦ä¹ åˆå­¦è€…éœ€è¦æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®è·µæŠ€å·§ã€‚é€šè¿‡ç³»ç»ŸåŒ–çš„å­¦ä¹ å’Œå®è·µï¼Œæ‚¨å¯ä»¥è·å¾—è¶…è¿‡99%çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œå¹¶ä¸ºåç»­æ›´å¤æ‚çš„æ·±åº¦å­¦ä¹ é¡¹ç›®å¥ å®šåšå®åŸºç¡€ã€‚

**å…³é”®è¦ç‚¹å›é¡¾:**
1. **æ•°æ®è´¨é‡æ˜¯åŸºç¡€**: å……åˆ†çš„æ•°æ®é¢„å¤„ç†å’Œå¢å¼º
2. **æ¨¡å‹é€‰æ‹©å¾ˆé‡è¦**: CNNé€‚åˆå›¾åƒä»»åŠ¡ï¼Œå‚æ•°å…±äº«æ•ˆç‡é«˜
3. **è¶…å‚æ•°éœ€è°ƒä¼˜**: å­¦ä¹ ç‡ã€æ‰¹å¤§å°ã€ç½‘ç»œç»“æ„éƒ½éœ€è¦å®éªŒéªŒè¯
4. **è¿‡æ‹Ÿåˆè¦é¢„é˜²**: ä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯å’Œæ—©åœç­–ç•¥
5. **å®éªŒç®¡ç†ä¸å¯å°‘**: ç³»ç»Ÿè®°å½•å’Œåˆ†æå®éªŒç»“æœ

ç»§ç»­ç»ƒä¹ å’Œæ¢ç´¢ï¼Œç¥æ‚¨åœ¨æ·±åº¦å­¦ä¹ é“è·¯ä¸Šå–å¾—æ›´å¤§è¿›æ­¥ï¼ ğŸš€